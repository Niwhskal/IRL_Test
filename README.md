#Apprenticeship IRL for navigation in the presence of moving objects

**note: This repository is an modification and extension of the work carried out by Jangir rishabh (https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/). 

The environment is modified to include moving obstacles and the agent was trained to traverse the path by user demonstrations along a pre-defined path. The policies recovered can be found in the 'saved_models-bumping' folder.

This project was carried in a bid to test the current state of the art cost recovering algorithms in the inverse optimal control paradigm. Our goal was to understand key aspects of these algorithms and develop on them to develop a self navigating agent in stochastic environments. A detailed report of all the algorithms surveyed can be found in the 'Report' folder in this repository. 


##Environment 

![alt text](https://raw.githubusercontent.com/test13234/IRL_test/Images/Env.png)

The rotatory arms induce stochasticity for the random environment. 

## Results
 
A screengrab of the best policy trained for can be found in the 'video' folder in the repository

**The best policy trained was 'policy_2', which can be found in the 'saved_models-bumping' folder, you can run the same (provided you satisfy all dependencies) using: 

`python3 playing.py 'bumping' '2' '100000'`


##Contact

If there are any queries regarding the implementation, you can reach me at shreeshalakshwin@gmail.com

##References and thanks

1. The algorithms for IRL were developed by Jangir rishabh and Matt harvey
2. Andrew Ng and Stuart Russell, 2000 - Algorithms for Inverse Reinforcement Learning
3. Pieter Abbeel and Andrew Ng, 2004 - Apprenticeship Learning via Inverse Reinforcement Learning
4. Mazimum-Entropy IRL, Ziebert Et.al , 2008
5. Guided cost Learning, Chelsea Finn, 2016


**Research carried out during October - December, 2018 

